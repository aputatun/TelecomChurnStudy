{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#### Preliminary Wrangling",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom IPython.display import HTML\nimport piplite\n%pip install seaborn\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nimport re\nimport matplotlib.pyplot as plt\nimport random",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv('train.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# high-level overview of data shape and composition\nprint(\"Number of Rows in day.csv : \",df.shape[0],\"\\nNumber of columns in day.csv : \",df.shape[1])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(\"\\n DataSet : Train.csv \\n\")\ndf.head(1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#### Data Dictionary",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_dictionary = pd.read_csv('data_dictionary.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data_dictionary",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#### Assessing",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Check duplicated value\ndisplay(HTML(f\"Number of duplicated values in the dataset : {df.duplicated().sum()}\"))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check null value for each column\nnull_cols = df.columns[df.isnull().all(axis=0)].tolist()\n\nprint(f\"List of columns with NULL's : \\n\\n {null_cols} \\n\")\nprint(f\"Count of columns having all NULL values : {len(null_cols)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "uniq_list = df.columns[(df.nunique() == 1)].tolist()\nprint(\"List of columns that have same value for all records : \\n\\n\", uniq_list )\nprint(\"\\nCount of columns that have same value for all records : \", len(uniq_list) )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "##### Removing all the columns having the same value from the dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Finding Columns with more than 50% of the data as NAN\nnan_columns  = df.columns[round(100* (df.isnull().sum()/len(df.index)),2) > 50].tolist()\nprint(\"List of columns that have null records for more than 50% of the data : \\n\\n\", nan_columns )\nprint(\"\\nCount of columns that have have null records for more than 50% of the data : \", len(nan_columns) )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#### Cleaning",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Remove columns that have same value for all records\n# First create a copy of the dataset to preserve original data\ndf_clean = df.copy()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean = df.drop(uniq_list,axis=1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Remove columns with more than 50% of the data as NAN\ndf_clean.drop(nan_columns,axis=1,inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "round(100* (df_clean.isnull().sum()/len(df.index)),2) ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Remove id column since its redundant\ndf_clean.drop(['id'],axis=1,inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.columns[round(100* (df_clean.isnull().sum()/len(df_clean.index)),2) > 0.0]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "### Imputing NAN values with their mean and mode for object columns\ndef impute_values (df):\n    nan_columns  = df.columns[round(100* (df.isnull().sum()/len(df.index)),2) > 0.0].tolist()\n    print(\"List of columns that have null records : \\n\\n\", nan_columns )\n    print(\"\\nCount of columns that have have null records  : \", len(nan_columns) )\n    object_cols = df.select_dtypes('object')\n    print(\"List of columns that are of object type : \\n\\n\", object_cols.columns.tolist() )\n    float_nan_columns = [x for x in nan_columns if x not in object_cols]\n    print(\"List of columns that are of float type : \\n\\n\", object_cols.columns.tolist() )\n    df[float_nan_columns].mean()\n    print(\"Mean of columns : \\n\\n\",df[float_nan_columns].mean())\n    df[float_nan_columns] = df[float_nan_columns].fillna(df[float_nan_columns].mean())\n    object_columns = object_cols.columns.tolist()\n    print(df[object_columns].mode())\n    for i in object_columns:\n        df[i] = df[i].fillna(df[i].mode()[0])\n    nan_columns  = df.columns[round(100* (df.isnull().sum()/len(df.index)),2) > 0.0].tolist()\n    print(\"List of columns that have null records : \\n\\n\", nan_columns )\n    \nimpute_values(df_clean)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "## Renaming the columns\ndf_clean = df_clean.rename(columns= lambda x : re.sub('_6$','june', x))\ndf_clean = df_clean.rename(columns= lambda x : re.sub('_7$','july', x))\ndf_clean = df_clean.rename(columns= lambda x : re.sub('_8$','august', x))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.head(1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean.head(1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "### Create three separate datasets for the three months to three different datasets\n\njune_columns  = [cols for cols in df_clean.columns if 'june' in cols]\ndf_clean_june = df_clean.loc[:,june_columns]\njuly_columns = [cols for cols in df_clean.columns if 'july' in cols]\ndf_clean_july = df_clean.loc[:,july_columns]\naugust_columns = [cols for cols in df_clean.columns if 'august' in cols]\ndf_clean_august = df_clean.loc[:,august_columns]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean_june.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean_july.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_clean_august.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Exploratory Data Analysis",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### __Univariate Analysis__ \n  → Mean, Median, Max, Min, Std, Variance, Count\n  → Distribution ( Histogram, CountPlot, BoxPlot)\n#### __Bivariate Analysis__\n  → Relationship Between 2 Variables ( ScatterPlot, BoxPlot, BarPlot etc)\n#### __Multivariate Analysis__\n  → Relationship Between more variables ( Heatmap etc.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "numerical_columns  = df_clean.select_dtypes('float64','int64')\nstring_columns = df_clean.select_dtypes('object')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}